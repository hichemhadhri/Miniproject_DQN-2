{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project 1: Deep Q-learning for Epidemic Mitigation\n",
    "\n",
    "### Authors : Mohamed Hichem hadhri, Yassine Chaouch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\" Seeding for reproducibility \"\"\"\n",
    "seed = 0\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logs(dyn, log):\n",
    "    \"\"\" Parse the logs \"\"\"\n",
    "    total = {p:np.array([getattr(l.total,p) for l in log]) for p in dyn.parameters}\n",
    "    cities = {c:{p:np.array([getattr(l.city[c],p) for l in log]) for p in dyn.parameters} for c in dyn.cities}\n",
    "    actions = {a:np.array([l.action[a] for l in log]) for a in log[0].action.keys()}\n",
    "    return total,cities,actions\n",
    "\n",
    "\n",
    "\n",
    "def plot_info(total, cities, actions, action_plot=True):\n",
    "    \"\"\" Plot the results \"\"\"\n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    ax_leftstate = plt.subplot2grid(shape=(9, 2), loc=(0, 0), rowspan=4)\n",
    "    ax_leftobs = plt.subplot2grid(shape=(9, 2), loc=(4, 0), rowspan=3)\n",
    "    if action_plot:\n",
    "        ax_leftactions = plt.subplot2grid(shape=(9, 2), loc=(7, 0), rowspan=2)\n",
    "    ax_right = [plt.subplot2grid(shape=(9, 2), loc=(0, 1), colspan=1)]\n",
    "    ax_right += [plt.subplot2grid(shape=(9, 2), loc=(i, 1), colspan=1) for i in range(1,9)]\n",
    "    ax_right = {k:ax_right[_id] for _id,k in enumerate(cities.keys())}\n",
    "\n",
    "    [ax_leftstate.plot(y) for y in total.values()]\n",
    "    ax_leftstate.legend(total.keys())\n",
    "    ax_leftstate.set_title('Full state')\n",
    "    ax_leftstate.set_ylabel('number of people in each state')\n",
    "\n",
    "    [ax_leftobs.plot(total[y]) for y in ['infected','dead']]\n",
    "    ax_leftobs.legend(['infected','dead'])\n",
    "    ax_leftobs.set_title('Observable state')\n",
    "    ax_leftobs.set_ylabel('number of people in each state')\n",
    "\n",
    "    if action_plot:\n",
    "        ax_leftactions.imshow(np.array([v for v in actions.values()]).astype(np.uint8),aspect='auto')\n",
    "        ax_leftactions.set_title('Actions')\n",
    "        ax_leftactions.set_yticks([0,1,2,3])\n",
    "        ax_leftactions.set_yticklabels(list(actions.keys()))\n",
    "        ax_leftactions.set_xlabel('time (in weeks)')\n",
    "\n",
    "\n",
    "    [ax.plot(cities[c]['infected']) for c, ax in ax_right.items()]\n",
    "    [ax.plot(cities[c]['dead']) for c, ax in ax_right.items()]\n",
    "    [ax.set_ylabel(c) for c, ax in ax_right.items()]\n",
    "    [ax.xaxis.set_major_locator(plt.NullLocator()) for c, ax in ax_right.items()]\n",
    "    ax_right['Zürich'].set_xlabel('time (in weeks)')\n",
    "    ax_right['Zürich'].xaxis.set_major_locator(MultipleLocator(2.000))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def hist_avg(ax, data,title):\n",
    "    ymax = 50\n",
    "    if title == 'deaths':\n",
    "        x_range = (1000,200000)\n",
    "    elif title == 'cumulative rewards': \n",
    "        x_range = (-300,300)\n",
    "    elif 'days' in title:\n",
    "        x_range = (0,200)\n",
    "    else:\n",
    "        raise ValueError(f'{title} is not a valid title') \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(0,ymax)\n",
    "    ax.vlines([np.mean(data)],0,ymax,color='red')\n",
    "    ax.hist(data,bins=60,range=x_range)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.a)** study the behavior of the model when epidemics are unmitigated <br>\n",
    "\n",
    "Run the epidemic simulation for one episode (30 weeks), without epidemic mitigation (meaning no action is taken, i.e. all values in the action dictionary are set to False) and produce three plots:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load the switzerland map\n",
    "dyn = ModelDynamics('config/switzerland.yaml') \n",
    "\n",
    "# create environment\n",
    "env = Env(dyn, action_space=None, observation_space=None) \n",
    "\n",
    "# DO NOTHING\n",
    "action = { \n",
    "    'confinement': False, \n",
    "    'isolation': False, \n",
    "    'hospital': False, \n",
    "    'vaccinate': False,\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\" Run the simulations, seeded\"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed)\n",
    "for t in range(30):\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) # save the information dict for logging\n",
    "    if finished:\n",
    "        break\n",
    "\n",
    "total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "\n",
    "plot_info(total, cities,actions, action_plot=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can clearly see the rate of mortality is high, e.g the number of deaths increases explonentially at the first 15 weeks, and the it converges sloowly to amaximum. Moreover, the number of Infected people increases exponentially at first and then it stabilizes for some weeksm until it drops qucikly to zero. This can be xplained that infected people can turn to either dead or recovered and at the end all the population is either dead or recovered. <br>\n",
    "\n",
    "Fron the results of each city, we can observe a trend. The number of infected people peaks around 4 to 6 weeks, followed by an increaed rate of mortality, This is due to the absence of actions to reduce the arete of spread and the rate of mortality like Isolation and hospitality <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Professor Russo’s Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.a)** Implement Pr. Russo’s Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from  Russo import Russo\n",
    "from utils import Utils\n",
    "\n",
    "dyn = ModelDynamics('config/switzerland.yaml') #load the switzerland map\n",
    "\n",
    "\n",
    "\n",
    "env = Env(dyn, action_space=None, observation_space=None, action_preprocessor=Utils.action_preprocessor) \n",
    "\n",
    "\n",
    "\n",
    "agent = Russo(env)\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed)\n",
    "agent.reset()\n",
    "\n",
    "while not finished:\n",
    "    action = agent.act(obs)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "    log.append(info) \n",
    "   \n",
    "total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "\n",
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.b)** Evaluate Pr. Russo’s Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml') #load the switzerland map\n",
    "\n",
    "\n",
    "\n",
    "env = Env(dyn, action_space=None, observation_space=None, action_preprocessor=Utils.action_preprocessor) # create environment\n",
    "\n",
    "agent = Russo(env)\n",
    "\n",
    "nb_all_confinement = []\n",
    "nb_all_rewards = []\n",
    "nb_all_deaths = []\n",
    "\n",
    "\"\"\"Run the simulations, seeded\"\"\"\n",
    "for trace in range(50):\n",
    "    log = []\n",
    "    finished = False\n",
    "    Rs = []\n",
    "    obs, info = env.reset(seed+trace)\n",
    "   \n",
    "    agent.reset()\n",
    "    for t in range(30):\n",
    "        action = agent.act(obs)\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        log.append(info) # save the information dict for logging\n",
    "        Rs.append(R)\n",
    "        if finished:\n",
    "            break\n",
    "   \n",
    "    total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "    # compute the number of days in confinement\n",
    "    nb_comfinement = np.sum(actions['confinement']) * 7\n",
    "    nb_all_confinement.append(nb_comfinement)\n",
    "\n",
    "    # compute the cumulative reward\n",
    "    cumulative_rewards = sum(Rs)\n",
    "    nb_all_rewards.append(int(cumulative_rewards))\n",
    "\n",
    "    # compute the number of deaths\n",
    "    nb_deaths = total['dead'][-1]\n",
    "    nb_all_deaths.append(nb_deaths)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "fig, ax = plt.subplots(3,figsize=(18,8))\n",
    "\n",
    "hist_avg(ax[0], nb_all_deaths,'deaths')\n",
    "hist_avg(ax[1], nb_all_rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], nb_all_confinement,'confined days')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print \"\"\"\n",
    "print(f'Average death number: {np.mean(nb_all_deaths)}')\n",
    "print(f'Average number of confined days: {np.mean(nb_all_confinement)}')\n",
    "print(f'Average cumulative reward: {np.mean(nb_all_rewards)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 A Deep Q-learning approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Deep Q-Learning with a binary action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.a)** implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from QLearning import QAgent\n",
    "from utils import Utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\" Seeding for reproducibility \"\"\"\n",
    "seed = 0\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.9\n",
    "EPS = 0.7\n",
    "LR = 5e-3\n",
    "\n",
    "# seed the random number generator for reproductibility\n",
    "random.seed(seed)\n",
    "\n",
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the dynamics\n",
    "dyn = ModelDynamics('config/switzerland.yaml') \n",
    "\n",
    "# define the action space\n",
    "action_space  =   spaces.Discrete(2)\n",
    "\n",
    "# define the observation space\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "# Define the environment\n",
    "env = Env(dyn, action_space=action_space, observation_space=observation_space, action_preprocessor=Utils.action_preprocessor,observation_preprocessor=Utils.observation_preprocessor)\n",
    "\n",
    "# Define the agent\n",
    "agent = QAgent(env, device , 2* dyn.n_cities* dyn.env_step_length)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=LR)\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 500\n",
    "\n",
    "# Define the number of evaluation episodes\n",
    "num_eval_episodes = 20\n",
    "\n",
    "# Define the training and evaluation traces\n",
    "train_trace = np.zeros(num_episodes)\n",
    "eval_trace = np.zeros(num_episodes)\n",
    "\n",
    "# Loop over the episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Run an episode\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "   \n",
    "    while not done:\n",
    "        action = agent.act(obs, eps=EPS)\n",
    "        next_obs, R, done, info = env.step(action)\n",
    "        R = torch.tensor(R, device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_obs = None\n",
    "        else:\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "      \n",
    "       \n",
    "        agent.remember(obs, action, next_obs,R )\n",
    "        obs = next_obs\n",
    "        cumulative_reward += int(R)\n",
    "    \n",
    "    # Log the cumulative reward to the training trace\n",
    "    train_trace[episode] = int(cumulative_reward)\n",
    "    \n",
    "    # Run a traing step\n",
    "    agent.optimize_model(optimizer,episode=episode)\n",
    "   \n",
    "   \n",
    "    \n",
    "    # Run an evaluation procedure every 50 episodes or on the last episode\n",
    "    if episode % 50 == 0 or episode == num_episodes - 1:\n",
    "        eval_cumulative_reward = 0\n",
    "        for eval_episode in range(num_eval_episodes):\n",
    "            obs, info = env.reset()\n",
    "            done = False\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "            while not done:\n",
    "                action = agent.act(obs, eps=0.0)\n",
    "                obs, R, done, info = env.step(action)\n",
    "                eval_cumulative_reward += R\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        eval_average_reward = int(eval_cumulative_reward / num_eval_episodes)\n",
    "        \n",
    "        # Log the evaluation results to the evaluation trace\n",
    "        eval_trace[episode] = eval_average_reward\n",
    "        \n",
    "        print(f\"Episode: {episode}, Train Reward: {int(cumulative_reward):.2f}, Eval Reward: {eval_average_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed)\n",
    "\n",
    "while not finished:\n",
    "       \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "       \n",
    "    action = agent.act(obs, eps=0.0)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "               \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "   \n",
    "    log.append(info) # save the information dict for logging\n",
    "  \n",
    "    \n",
    "total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "\n",
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from QLearning import QAgent\n",
    "from utils import Utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "import math\n",
    "\n",
    "\"\"\" Seeding for reproducibility \"\"\"\n",
    "seed = 0\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.9\n",
    "EPS = 0.7\n",
    "LR = 5e-3\n",
    "\n",
    "# seed the random number generator for reproductibility\n",
    "random.seed(seed)\n",
    "\n",
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the dynamics\n",
    "dyn = ModelDynamics('config/switzerland.yaml') \n",
    "\n",
    "# define the action space\n",
    "action_space  =   spaces.Discrete(2)\n",
    "\n",
    "# define the observation space\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "# Define the environment\n",
    "env = Env(dyn, action_space=action_space, observation_space=observation_space, action_preprocessor=Utils.action_preprocessor,observation_preprocessor=Utils.observation_preprocessor)\n",
    "\n",
    "# Define the agent\n",
    "agent = QAgent(env, device , 2* dyn.n_cities* dyn.env_step_length)\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=LR)\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 500\n",
    "\n",
    "# Define the number of evaluation episodes\n",
    "num_eval_episodes = 20\n",
    "\n",
    "# define the number of traces\n",
    "num_traces = 3\n",
    "\n",
    "# Define the training and evaluation traces\n",
    "train_trace = np.zeros((num_traces,num_episodes))\n",
    "eval_trace = np.zeros((num_traces,num_episodes))\n",
    "\n",
    "# best reward\n",
    "best_reward = -math.inf \n",
    "\n",
    "for trace in range(num_traces):\n",
    "\n",
    "    agent.reset()\n",
    "    optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=LR)\n",
    "    # Loop over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        done = False\n",
    "        cumulative_reward = 0\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(obs, eps=EPS,eps_decay=True,episode=episode,num_episodes=num_episodes,eps_min=0.2)\n",
    "            next_obs, R, done, info = env.step(action)\n",
    "            R = torch.tensor(R, device=device)\n",
    "            \n",
    "            if done:\n",
    "                next_obs = None\n",
    "            else:\n",
    "                next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        \n",
    "        \n",
    "            agent.remember(obs, action, next_obs,R )\n",
    "            obs = next_obs\n",
    "            cumulative_reward += int(R)\n",
    "        \n",
    "        # Log the cumulative reward to the training trace\n",
    "        train_trace[trace,episode] = int(cumulative_reward)\n",
    "        \n",
    "        # Run a traing step\n",
    "        agent.optimize_model(optimizer,episode=episode)\n",
    "\n",
    "\n",
    "        \n",
    "        # Run an evaluation procedure every 50 episodes or on the last episode\n",
    "        if episode % 50 == 0 or episode == num_episodes - 1:\n",
    "            eval_cumulative_reward = 0\n",
    "            for eval_episode in range(num_eval_episodes):\n",
    "                obs, info = env.reset()\n",
    "                done = False\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "                while not done:\n",
    "                    action = agent.act(obs, eps=0.0)\n",
    "                    obs, R, done, info = env.step(action)\n",
    "                    eval_cumulative_reward += R\n",
    "                    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "                \n",
    "                if eval_cumulative_reward > best_reward:\n",
    "                    agent.save_model(savepath='models/best_model')\n",
    "                    best_reward = eval_cumulative_reward\n",
    "\n",
    "\n",
    "            eval_average_reward = int(eval_cumulative_reward / num_eval_episodes)\n",
    "            \n",
    "            # Log the evaluation results to the evaluation trace\n",
    "            eval_trace[trace,episode] = eval_average_reward\n",
    "            \n",
    "            print(f\" Trace {trace }Episode: {episode}, Train Reward: {int(cumulative_reward):.2f}, Eval Reward: {eval_average_reward:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace zeros in the evaluation trace with the last non-zero value (we evaluate every 50 episodes)\n",
    "for t in range(num_traces):\n",
    "    for e in range(num_episodes):\n",
    "        if eval_trace[t,e] == 0:\n",
    "            eval_trace[t,e] = eval_trace[t,e-1]\n",
    "\n",
    "\n",
    "# plot the training and evaluation traces\n",
    "plt.figure(figsize=(10,5))\n",
    "# change opactiy to 0.5 for a more transparent plot\n",
    "colors = ['green','blue','orange']\n",
    "\n",
    "[plt.scatter(np.arange(num_episodes),train_trace[t],label='Train',alpha=0.5,color=colors[t]) for t in range(num_traces)]\n",
    "plt.plot(np.arange(num_episodes),np.mean(eval_trace,axis=0),label='Eval',color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed)\n",
    "\n",
    "while not finished:\n",
    "       \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "       \n",
    "    action = agent.act(obs, eps=0.0)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "               \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "   \n",
    "    log.append(info) # save the information dict for logging\n",
    "  \n",
    "    \n",
    "total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "\n",
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml') #load the switzerland map\n",
    "\n",
    "\n",
    "\n",
    "action_space  =   spaces.Discrete(2)\n",
    "\n",
    "# define the observation space\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "# Define the environment\n",
    "env = Env(dyn, action_space=action_space, observation_space=observation_space, action_preprocessor=Utils.action_preprocessor,observation_preprocessor=Utils.observation_preprocessor)\n",
    "\n",
    "# Define the agent\n",
    "agent = QAgent(env, device , 2* dyn.n_cities* dyn.env_step_length)\n",
    "\n",
    "agent.load_model('models/best_model')\n",
    "\n",
    "nb_all_confinement = []\n",
    "nb_all_rewards = []\n",
    "nb_all_deaths = []\n",
    "\n",
    "\"\"\"Run the simulations, seeded\"\"\"\n",
    "for trace in range(50):\n",
    "    log = []\n",
    "    finished = False\n",
    "    Rs = []\n",
    "    obs, info = env.reset(seed+trace)\n",
    "   \n",
    "\n",
    "    for t in range(30):\n",
    "        action = agent.act(obs, eps=0.0)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        obs, R, finished, info = env.step(action)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "        log.append(info) # save the information dict for logging\n",
    "        Rs.append(R)\n",
    "        if finished:\n",
    "            break\n",
    "   \n",
    "    total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "    # compute the number of days in confinement\n",
    "    nb_comfinement = np.sum(actions['confinement']) * 7\n",
    "    nb_all_confinement.append(nb_comfinement)\n",
    "\n",
    "    # compute the cumulative reward\n",
    "    cumulative_rewards = sum(Rs)\n",
    "    nb_all_rewards.append(int(cumulative_rewards))\n",
    "\n",
    "    # compute the number of deaths\n",
    "    nb_deaths = total['dead'][-1]\n",
    "    nb_all_deaths.append(nb_deaths)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "fig, ax = plt.subplots(3,figsize=(18,8))\n",
    "\n",
    "hist_avg(ax[0], nb_all_deaths,'deaths')\n",
    "hist_avg(ax[1], nb_all_rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], nb_all_confinement,'confined days')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print \"\"\"\n",
    "print(f'Average death number: {np.mean(nb_all_deaths)}')\n",
    "print(f'Average number of confined days: {np.mean(nb_all_confinement)}')\n",
    "print(f'Average cumulative reward: {np.mean(nb_all_rewards)}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 : Dealing with a more complex action Space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Toggle-action-space multi-action agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from constants import *\n",
    "from utils import Utils\n",
    "from epidemic_env.agent import Agent\n",
    "from QLearningModel import QLearningModel , ReplayMemory\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "class QAgentMulti(Agent):\n",
    "\n",
    "    def __init__(self, env,device,input_size):\n",
    "        self.env = env\n",
    "\n",
    "        self.policy_network = QLearningModel(input_size=input_size)\n",
    "        self.target_network = QLearningModel(input_size=input_size)\n",
    "        self.input_size = input_size\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def load_model(self, savepath):\n",
    "\n",
    "        self.policy_network.load_state_dict(torch.load(savepath+str(\"policy.pt\")))\n",
    "        self.target_network.load_state_dict(torch.load(savepath+str(\"target.pt\")))\n",
    "        return\n",
    "      \n",
    "        \n",
    "\n",
    "    def save_model(self, savepath):\n",
    "        torch.save(self.policy_network.state_dict(), savepath+str(\"policy\")+str(\".pt\"))\n",
    "        torch.save(self.target_network.state_dict(), savepath+str(\"target\")+str(\".pt\"))\n",
    "        return\n",
    "\n",
    "    def remember(self, state, action, next_state, reward):\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "\n",
    "    def optimize_model(self, optimizer,episode,gamma=0.9):\n",
    "\n",
    "        batch_size = BATCH_SIZE\n",
    "        # if memory is smaller than batch size, do nothing\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        \n",
    "        # sample a batch from Memory\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "\n",
    "        batch = Utils.Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        \n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy_network(torch.flatten(state_batch, start_dim=1)).gather(1, action_batch)\n",
    "\n",
    "       \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(torch.flatten(non_final_next_states, start_dim=1)).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * gamma) + reward_batch.squeeze(1)\n",
    "       \n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # every 5 episodes update the target network\n",
    "        if episode % 5 == 0:\n",
    "            self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        \n",
    "       \n",
    "\n",
    "       \n",
    "    \n",
    "    def reset(self):\n",
    "        # This should be called when the environment is reset\n",
    "        self.policy_network = QLearningModel(input_size=self.input_size)\n",
    "        self.target_network = QLearningModel(input_size=self.input_size)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "    def act(self, obs,eps,eps_decay=False,eps_min=None,num_episodes=None,episode=None):\n",
    "        # this takes an observation and returns an action\n",
    "        # the action space can be directly sampled from the env\n",
    "        sample = random.random()\n",
    "        if eps_decay : \n",
    "            eps_threshold = max(eps_min,eps *(num_episodes - episode ) / num_episodes)\n",
    "        else : \n",
    "            eps_threshold = eps\n",
    "\n",
    "        if sample < 1 - eps_threshold:\n",
    "            with torch.no_grad():\n",
    "               \n",
    "                \n",
    "                return self.policy_network(torch.flatten(obs, start_dim=1)).max(1)[1].view(1, 1)\n",
    "      \n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], device=self.device, dtype=torch.long)\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/jsb8y92n26zf96f62ppq83sw0000gn/T/ipykernel_8947/790455469.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
      "/var/folders/yv/jsb8y92n26zf96f62ppq83sw0000gn/T/ipykernel_8947/790455469.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R = torch.tensor(R, device=device)\n",
      "/var/folders/yv/jsb8y92n26zf96f62ppq83sw0000gn/T/ipykernel_8947/790455469.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
      "/var/folders/yv/jsb8y92n26zf96f62ppq83sw0000gn/T/ipykernel_8947/790455469.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  agent.remember(obs, torch.tensor(memory_action,dtype=torch.int64), next_obs,R )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 131])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m train_trace[trace,episode] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(cumulative_reward)\n\u001b[1;32m    128\u001b[0m \u001b[39m# Run a traing step\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m agent\u001b[39m.\u001b[39;49moptimize_model(optimizer,episode\u001b[39m=\u001b[39;49mepisode)\n\u001b[1;32m    133\u001b[0m \u001b[39m# Run an evaluation procedure every 50 episodes or on the last episode\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m episode \u001b[39m==\u001b[39m num_episodes \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/Miniproject_DQN 2/QLearningMulti.py:72\u001b[0m, in \u001b[0;36mQAgentMulti.optimize_model\u001b[0;34m(self, optimizer, episode, gamma)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(state_batch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     69\u001b[0m \u001b[39m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m# columns of actions taken. These are the actions which would've been taken\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# for each batch state according to policy_net\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m state_action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_network(state_batch)\u001b[39m.\u001b[39;49mgather(\u001b[39m1\u001b[39;49m, action_batch)\n\u001b[1;32m     75\u001b[0m \u001b[39m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# on the \"older\" target_net; selecting their best reward with max(1)[0].\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m# This is merged based on the mask, such that we'll have either the expected\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# state value or 0 in case the state was final.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m next_state_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batch_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from QLearningMulti import QAgentMulti\n",
    "from utils import Utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"Environment imports\"\"\"\n",
    "from epidemic_env.env       import Env, Log\n",
    "from epidemic_env.dynamics  import ModelDynamics, Observation\n",
    "from epidemic_env.visualize import Visualize\n",
    "from epidemic_env.agent     import Agent\n",
    "\n",
    "\"\"\"Pytorch and numpy imports\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "import math\n",
    "\n",
    "\"\"\" Seeding for reproducibility \"\"\"\n",
    "seed = 0\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "GAMMA = 0.9\n",
    "EPS = 0.7\n",
    "LR = 1e-5\n",
    "\n",
    "# seed the random number generator for reproductibility\n",
    "random.seed(seed)\n",
    "\n",
    "# set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the dynamics\n",
    "dyn = ModelDynamics('config/switzerland.yaml') \n",
    "\n",
    "# define the action space\n",
    "action_space  =   spaces.Discrete(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "env = Env(dyn, action_space=action_space, observation_space=None, action_preprocessor=Utils.action_preprocessor,observation_preprocessor=Utils.observation_preprocessor_action)\n",
    "\n",
    "\n",
    "# Define the agent\n",
    "agent = QAgentMulti(env, device , dyn.n_cities * 2 * dyn.env_step_length +5 )\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=LR)\n",
    "\n",
    "# Define the number of episodes\n",
    "num_episodes = 500\n",
    "\n",
    "# Define the number of evaluation episodes\n",
    "num_eval_episodes = 20\n",
    "\n",
    "# define the number of traces\n",
    "num_traces = 3\n",
    "\n",
    "# Define the training and evaluation traces\n",
    "train_trace = np.zeros((num_traces,num_episodes))\n",
    "eval_trace = np.zeros((num_traces,num_episodes))\n",
    "\n",
    "# best reward\n",
    "best_reward = -math.inf \n",
    "\n",
    "for trace in range(num_traces):\n",
    "    \n",
    "    agent.reset()\n",
    "\n",
    "    optimizer = torch.optim.Adam(agent.policy_network.parameters(), lr=LR)\n",
    "    # Loop over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode\n",
    "        obs, info = env.reset(seed=seed+episode)\n",
    "\n",
    "        \n",
    "        done = False\n",
    "        cumulative_reward = 0\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    \n",
    "      \n",
    "        while not done:\n",
    "            action = agent.act(obs, eps=EPS,eps_decay=True,episode=episode,num_episodes=num_episodes,eps_min=0.2)\n",
    "           \n",
    "            next_obs, R, done, info = env.step(action)\n",
    "\n",
    "        \n",
    "\n",
    "            memory_action =  [int(v) for v in info.action.values()]\n",
    "            # inver last two elements of action because vaccinate is action with code 3 (kalamni nfasarlek)\n",
    "            memory_action[2], memory_action[3] = memory_action[3], memory_action[2]\n",
    "\n",
    "        # prepend 0 if no action (to set the NO_ACTION)\n",
    "            if sum(memory_action) == 0:\n",
    "                memory_action = [1] + memory_action\n",
    "            else:\n",
    "                memory_action = [0] + memory_action\n",
    "\n",
    "            # get indices of actions set to 1\n",
    "            memory_action = [i for i, x in enumerate(memory_action) if x == 1]\n",
    "            \n",
    "            R = torch.tensor(R, device=device)\n",
    "            \n",
    "            if done:\n",
    "                next_obs = None\n",
    "            else:\n",
    "                next_obs = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "            memory_action = torch.tensor(memory_action, dtype=torch.float32, device=device)\n",
    "\n",
    "        \n",
    "            agent.remember(obs, torch.tensor([memory_action],dtype=torch.int64).unsqueeze(0), next_obs,R )\n",
    "            obs = next_obs\n",
    "            cumulative_reward += int(R)\n",
    "         \n",
    "        # Log the cumulative reward to the training trace\n",
    "        train_trace[trace,episode] = int(cumulative_reward)\n",
    "        \n",
    "        # Run a traing step\n",
    "        agent.optimize_model(optimizer,episode=episode)\n",
    "\n",
    "\n",
    "        \n",
    "        # Run an evaluation procedure every 50 episodes or on the last episode\n",
    "        if episode % 50 == 0 or episode == num_episodes - 1:\n",
    "            eval_cumulative_reward = 0\n",
    "            for eval_episode in range(num_eval_episodes):\n",
    "                obs, info = env.reset()\n",
    "                done = False\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "                while not done:\n",
    "                    action = agent.act(obs, eps=0.0)\n",
    "               \n",
    "                    obs, R, done, info = env.step(action)\n",
    "                    eval_cumulative_reward += R\n",
    "                    obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "                \n",
    "                if eval_cumulative_reward > best_reward:\n",
    "                    agent.save_model(savepath='models/best_model_multi')\n",
    "                    best_reward = eval_cumulative_reward\n",
    "\n",
    "\n",
    "            eval_average_reward = int(eval_cumulative_reward / num_eval_episodes)\n",
    "            \n",
    "            # Log the evaluation results to the evaluation trace\n",
    "            eval_trace[trace,episode] = eval_average_reward\n",
    "            \n",
    "            print(f\" Trace {trace} Episode: {episode}, Train Reward: {int(cumulative_reward):.2f}, Eval Reward: {eval_average_reward:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward /20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = ModelDynamics('config/switzerland.yaml') #load the switzerland map\n",
    "\n",
    "\n",
    "\n",
    "action_space  =   spaces.Discrete(5)\n",
    "\n",
    "# define the observation space\n",
    "observation_space   =   spaces.Box( low=0,\n",
    "                                    high=1,\n",
    "                                    shape=(2, dyn.n_cities, dyn.env_step_length),\n",
    "                                    dtype=np.float16)\n",
    "# Define the environment\n",
    "env = Env(dyn, action_space=action_space, observation_space=observation_space, action_preprocessor=Utils.action_preprocessor,observation_preprocessor=Utils.observation_preprocessor_action)\n",
    "\n",
    "# Define the agent\n",
    "agent = QAgentMulti(env, device , dyn.n_cities * 2 * dyn.env_step_length +5 )\n",
    "\n",
    "agent.load_model('models/best_model_multi')\n",
    "\n",
    "nb_all_confinement = []\n",
    "nb_all_rewards = []\n",
    "nb_all_deaths = []\n",
    "\n",
    "\"\"\"Run the simulations, seeded\"\"\"\n",
    "for trace in range(50):\n",
    "    log = []\n",
    "    finished = False\n",
    "    Rs = []\n",
    "    obs, info = env.reset(seed+trace)\n",
    "   \n",
    "    \n",
    "\n",
    "    for t in range(30):\n",
    "\n",
    "        action = agent.act(obs, eps=0.0)\n",
    "        \n",
    "        obs, R, finished, info = env.step(action)\n",
    "        log.append(info) # save the information dict for logging\n",
    "        Rs.append(R)\n",
    "        if finished:\n",
    "            break\n",
    "   \n",
    "    total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "    # compute the number of days in confinement\n",
    "    nb_comfinement = np.sum(actions['confinement']) * 7\n",
    "    nb_all_confinement.append(nb_comfinement)\n",
    "\n",
    "    # compute the cumulative reward\n",
    "    cumulative_rewards = sum(Rs)\n",
    "    nb_all_rewards.append(int(cumulative_rewards))\n",
    "\n",
    "    # compute the number of deaths\n",
    "    nb_deaths = total['dead'][-1]\n",
    "    nb_all_deaths.append(nb_deaths)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "fig, ax = plt.subplots(3,figsize=(18,8))\n",
    "\n",
    "hist_avg(ax[0], nb_all_deaths,'deaths')\n",
    "hist_avg(ax[1], nb_all_rewards,'cumulative rewards')\n",
    "hist_avg(ax[2], nb_all_confinement,'confined days')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\" Print \"\"\"\n",
    "print(f'Average death number: {np.mean(nb_all_deaths)}')\n",
    "print(f'Average number of confined days: {np.mean(nb_all_confinement)}')\n",
    "print(f'Average cumulative reward: {np.mean(nb_all_rewards)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Run the simulation \"\"\"\n",
    "log = []\n",
    "finished = False\n",
    "obs, info = env.reset(seed)\n",
    "actions_ = []\n",
    "while not finished:\n",
    "       \n",
    "       \n",
    "    action = agent.act(obs, eps=0.0)\n",
    "    actions_.append(action)\n",
    "    obs, R, finished, info = env.step(action)\n",
    "               \n",
    "   \n",
    "    log.append(info) \n",
    "  \n",
    "    \n",
    "total, cities, actions = parse_logs(dyn, log)\n",
    "\n",
    "\n",
    "plot_info(total, cities, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
